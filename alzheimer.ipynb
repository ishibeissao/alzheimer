{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout\n",
    "import os\n",
    "import random\n",
    "from IPython.display import display\n",
    "from matplotlib import image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(dir_path):\n",
    "    return os.listdir(dir_path)\n",
    "\n",
    "def data_analysis_histogram(dir_path, classes, verbose = 1):\n",
    "    class_dist = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        class_dist.append(len(os.listdir(class_path)))\n",
    "    \n",
    "    if verbose > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"Class distribution\")\n",
    "        plt.barh(classes, class_dist)\n",
    "        for index, value in enumerate(class_dist):\n",
    "            plt.text(value, index,str(value))\n",
    "        plt.show()\n",
    "\n",
    "def data_analysis_image_size(dir_path, classes, verbose = 1, seed = -1):\n",
    "    if seed>=0:\n",
    "        random.seed(seed)\n",
    "    random_class_path = os.path.join(dir_path,random.choice(classes))\n",
    "    random_img_name = random.choice(os.listdir(random_class_path))\n",
    "    random_img_path = os.path.join(random_class_path,random_img_name)\n",
    "    if verbose > 0:\n",
    "        img = image.imread(random_img_path)\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"%s - Height: %d px x Length: %d px\" % (random_img_path,img.shape[0],img.shape[1]))\n",
    "        plt.imshow(img)\n",
    "\n",
    "def analyse_dataset(dir_path, verbose = 1, seed = -1):\n",
    "    classes = get_classes(dir_path)\n",
    "    data_analysis_histogram(dir_path,classes, verbose)\n",
    "    data_analysis_image_size(dir_path,classes, verbose, seed)\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_path, verbose = 1):\n",
    "    classes = get_classes(dir_path)\n",
    "    img_array = []\n",
    "    class_array = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        imgs_name = os.listdir(class_path)\n",
    "        for i in imgs_name:\n",
    "            img_array.append(image.imread(os.path.join(class_path,i)))\n",
    "            class_array.append(c)\n",
    "    if verbose > 0:\n",
    "        print(\"Loaded %d images\" % len(img_array))\n",
    "    return np.array(img_array), np.array(class_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, val_size = 0.2, verbose = 1, seed = 42):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,  y, test_size=val_size, random_state=seed)\n",
    "    if verbose > 0:\n",
    "        print(\"Train size: %d\\nValidation size: %d\" % (len(x_train), len(x_val)))\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x , y , classes):\n",
    "    x_scaled = x.astype('float32') / 255.0\n",
    "    class_map = {x: i for i,x in enumerate(classes)}\n",
    "    y_code = [class_map[word] for word in y]\n",
    "    y_categorical = keras.utils.to_categorical(y_code, len(classes))\n",
    "\n",
    "    inv_class_map = {v: k for k, v in class_map.items()}\n",
    "    return x_scaled , y_categorical, inv_class_map, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = analyse_dataset('../Alzheimer_s Dataset/train', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5121 images\n"
     ]
    }
   ],
   "source": [
    "x, y = load_dataset('../Alzheimer_s Dataset/train',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4096\n",
      "Validation size: 1025\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = split_dataset(x, y, 0.2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prepared , y_train_prepared, class_map, _ = prepare_dataset(x_train , y_train , classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
