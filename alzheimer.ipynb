{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "from matplotlib import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from prettytable import PrettyTable \n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.metrics import auc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from PIL import Image\n",
    "#from sklearn.metrics import balanced_accuracy_score\n",
    "#from ipywidgets import widgets\n",
    "#rom pathlib import Path\n",
    "#from IPython.display import display\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(dir_path):\n",
    "    return os.listdir(dir_path)\n",
    "\n",
    "def data_analysis_histogram(dir_path, classes, verbose = 1):\n",
    "    class_dist = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        class_dist.append(len(os.listdir(class_path)))\n",
    "    \n",
    "    if verbose > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"Class distribution\")\n",
    "        plt.barh(classes, class_dist)\n",
    "        for index, value in enumerate(class_dist):\n",
    "            plt.text(value, index,str(value))\n",
    "        plt.show()\n",
    "    return class_dist\n",
    "\n",
    "def data_analysis_image_size(dir_path, classes, verbose = 1, seed = 42):\n",
    "    random.seed(seed)\n",
    "    random_class_path = os.path.join(dir_path,random.choice(classes))\n",
    "    random_img_name = random.choice(os.listdir(random_class_path))\n",
    "    random_img_path = os.path.join(random_class_path,random_img_name)\n",
    "    img = image.imread(random_img_path)\n",
    "    if verbose > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"%s - Height: %d px x Length: %d px\" % (random_img_path,img.shape[0],img.shape[1]))\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    return (img.shape[0],img.shape[1],1)\n",
    "\n",
    "def analyse_dataset(dir_path, verbose = 1, seed = 42):\n",
    "    classes = get_classes(dir_path)\n",
    "    class_dist = data_analysis_histogram(dir_path,classes, verbose)\n",
    "    input_shape = data_analysis_image_size(dir_path,classes, verbose, seed)\n",
    "    return classes, input_shape, class_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_path, percentage = 1, verbose = 1):\n",
    "    classes = get_classes(dir_path)\n",
    "    img_array = []\n",
    "    class_array = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        imgs_name = os.listdir(class_path)\n",
    "\n",
    "        if percentage < 1:\n",
    "            imgs_name = random.sample(imgs_name, k = int(len(imgs_name)*percentage))\n",
    "\n",
    "        for i in imgs_name:\n",
    "            img_array.append(image.imread(os.path.join(class_path,i)))\n",
    "            class_array.append(c)\n",
    "    if verbose > 0:\n",
    "        print(\"Loaded %d images\" % len(img_array))\n",
    "        \n",
    "    return np.array(img_array), np.array(class_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, val_size = 0.2, verbose = 1, seed = 42):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,  y, test_size=val_size, random_state=seed)\n",
    "    if verbose > 0:\n",
    "        print(\"Train size: %d\\nValidation size: %d\" % (len(x_train), len(x_val)))\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_channel_position(x, input_shape):\n",
    "    img_lin,img_col,n_channels = input_shape\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        x = x.reshape(x.shape[0], n_channels, img_lin, img_col)\n",
    "        input_shape = (n_channels, img_lin, img_col)\n",
    "    else:\n",
    "        x = x.reshape(x.shape[0], img_lin, img_col, n_channels)\n",
    "        input_shape = (img_lin, img_col, n_channels)\n",
    "    return x, input_shape\n",
    "\n",
    "def prepare_dataset_input(x, input_shape):\n",
    "    x_scaled = x.astype('float32') / 255.0\n",
    "    return prepare_dataset_channel_position(x_scaled, input_shape)\n",
    "\n",
    "def prepare_dataset_output(y, classes):\n",
    "    class_map = {x: i for i,x in enumerate(classes)}\n",
    "    y_code = [class_map[word] for word in y]\n",
    "    y_categorical = keras.utils.to_categorical(y_code, len(classes))\n",
    "    inv_class_map = {v: k for k, v in class_map.items()}\n",
    "    return y_categorical, inv_class_map\n",
    "\n",
    "def prepare_dataset(x , y , classes, input_shape):\n",
    "    x_scaled, input_shape = prepare_dataset_input(x, input_shape)\n",
    "    y_categorical, inv_class_map = prepare_dataset_output(y, classes)\n",
    "    return x_scaled , y_categorical, inv_class_map, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_execution_history(foldername, history, model, verbose, save = True, results_path = 'results'):\n",
    "    result_directory = os.path.join(results_path,foldername)\n",
    "    image_path = os.path.join(result_directory,'execution.png')\n",
    "\n",
    "    nplots = len(history.values())/2\n",
    "    nrows = int(nplots/3)\n",
    "    ncols = 3\n",
    "    fig = plt.figure(figsize=(ncols*8, nrows*5))\n",
    "    gs = fig.add_gridspec(nrows, ncols)\n",
    "    axs = gs.subplots(sharex=False, sharey=False)\n",
    "    for i,h in enumerate(history.values()):\n",
    "        i_hat = int(i%nplots)\n",
    "        r = int(i_hat/ncols)\n",
    "        c = i_hat%ncols\n",
    "        axs[r,c].plot(h, label = 'Training' if int(i/nplots)==0 else 'Validation')\n",
    "    \n",
    "    for i,n in enumerate(model.metrics_names):\n",
    "        r = int(i/ncols)\n",
    "        c = i%ncols\n",
    "        axs[r,c].set_xlabel('Epoch')\n",
    "        axs[r,c].set_ylabel(n)\n",
    "        axs[r,c].legend()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(image_path)\n",
    "    if verbose > 0:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evalution_roc(foldername, name, model, x, y, inv_class_map, verbose = 1, save = True, results_path = 'results'):\n",
    "    y_pred = model.predict(x)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(inv_class_map)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(inv_class_map)):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            label=\"ROC curve of {0} (AUC = {1:0.2f})\".format(inv_class_map[i], roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save:\n",
    "        result_directory = os.path.join(results_path,foldername)\n",
    "        image_path = os.path.join(result_directory,name+'_roc_curve.png')\n",
    "        plt.savefig(image_path)\n",
    "\n",
    "    if verbose > 0:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, elapsed_minutes, results_path = 'results'):\n",
    "    result_directory = os.path.join(results_path)\n",
    "\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "    \n",
    "    name = model.name\n",
    "\n",
    "    now = datetime.now()\n",
    "    name = model.name + now.strftime(\" [%Y-%m-%d-%H-%M-%S-%f]\")\n",
    "\n",
    "    result_directory = os.path.join(result_directory,name)\n",
    "\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "    else:\n",
    "        raise ValueError(\"File already exists.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model.save(model_path)\n",
    "\n",
    "    execution_path = os.path.join(result_directory,'execution')\n",
    "\n",
    "    execution = {\n",
    "            'epochs': history.params['epochs'],\n",
    "            'history': history.history,\n",
    "            'elapsed_minutes': elapsed_minutes\n",
    "    }\n",
    "\n",
    "    with open(execution_path, 'wb') as f:\n",
    "        pickle.dump(execution, f)\n",
    "    \n",
    "    pretty_model_path = os.path.join(result_directory, 'model_summary.txt')\n",
    "\n",
    "    with open(pretty_model_path,'w') as f:\n",
    "        f.write('Elapsed time: '+str(elapsed_minutes)+' min\\n')\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(foldername, results_path = 'results'):\n",
    "    result_directory = os.path.join(results_path,foldername)\n",
    "    if not os.path.exists(result_directory):\n",
    "        raise ValueError(\"Folder not found.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model = keras.models.load_model(model_path)\n",
    "\n",
    "    execution_path = os.path.join(result_directory,'execution')\n",
    "    execution = pickle.load(open(execution_path, \"rb\"))\n",
    "    \n",
    "    return model, execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation(foldername, name, all_scores, table, results_path = 'results'):\n",
    "    result_directory = os.path.join(results_path,foldername)\n",
    "    if not os.path.exists(result_directory):\n",
    "        raise ValueError(\"Folder not found.\")\n",
    "    \n",
    "    score_path = os.path.join(result_directory, name + '_score.json')\n",
    "    with open(score_path, 'w') as f:\n",
    "        json.dump(all_scores, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    pretty_score_path = os.path.join(result_directory, name + '_score_summary.txt')\n",
    "\n",
    "    with open(pretty_score_path,'w') as f:\n",
    "        f.write(foldername+'\\n')\n",
    "        f.write(table.get_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_by_class(model, x, y, verbose = 1):\n",
    "    def separate_by_class(x, y):\n",
    "        n_classes = y.shape[1]\n",
    "        x_classified = [[] for _ in range(n_classes)]\n",
    "        y_classified = [[] for _ in range(n_classes)]\n",
    "        \n",
    "        for i,img in enumerate(y):\n",
    "            index = np.where(img==1)[0][0]\n",
    "            x_classified[index].append(x[i])\n",
    "            y_classified[index].append(y[i])\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            x_classified[i] = np.array(x_classified[i])\n",
    "            y_classified[i] = np.array(y_classified[i])\n",
    "            \n",
    "        return np.array(x_classified,dtype=object), np.array(y_classified,dtype=object)\n",
    "\n",
    "    x_by_class, y_by_class = separate_by_class(x,y)\n",
    "    \n",
    "    score_by_class = []\n",
    "    for x,y in zip(x_by_class,y_by_class):\n",
    "        score = model.evaluate(x, y, verbose = verbose)\n",
    "        score_by_class.append(score)\n",
    "\n",
    "    return score_by_class\n",
    "\n",
    "def evaluate_model_confusion_matrix(foldername, name, model, x, y, inv_class_map, verbose = 1, save = True, results_path = 'results'):\n",
    "    def undoOneHotEncoding(y, inv_class_map):\n",
    "        return [inv_class_map[i] for i in np.argmax(y, axis = 1)]\n",
    "    \n",
    "    y_pred = model.predict(x)\n",
    "    y_pred_int = undoOneHotEncoding(y_pred, inv_class_map)\n",
    "    y_int = undoOneHotEncoding(y, inv_class_map)\n",
    "    cm = confusion_matrix(y_int, y_pred_int)\n",
    "\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                        index = [inv_class_map[i] for i in range(len(inv_class_map))], \n",
    "                        columns = [inv_class_map[i] for i in range(len(inv_class_map))])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    if save:\n",
    "        results_directory = os.path.join(results_path,foldername)\n",
    "        image_path = os.path.join(results_directory,name+'_confusion_matrix.png')\n",
    "        plt.savefig(image_path)\n",
    "    if verbose > 0:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    return cm\n",
    "\n",
    "def evaluate_model(foldername, name, model, x, y, inv_class_map, verbose = 1, save = True):\n",
    "    score = model.evaluate(x, y, verbose = verbose)\n",
    "    score_by_class = evaluate_model_by_class(model, x, y, verbose)\n",
    "    cm = evaluate_model_confusion_matrix(foldername, name, model,x, y, inv_class_map, verbose, save)\n",
    "    _, _, roc_auc = plot_evalution_roc(foldername, name, model, x, y, inv_class_map, verbose , save)\n",
    "    table = PrettyTable()\n",
    "\n",
    "    table.add_column(\"Metrics\", model.metrics_names)\n",
    "    table.add_column(\"Global\", np.round(score,4))\n",
    "\n",
    "    for i, s_class in enumerate(score_by_class):\n",
    "        table.add_column(inv_class_map[i], np.round(s_class,4))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print()\n",
    "        print(name)\n",
    "        print(table)\n",
    "\n",
    "    all_scores = {\n",
    "        'model_name': foldername,\n",
    "        'name': name,\n",
    "        'loss': score,\n",
    "        'loss_by_class': score_by_class,\n",
    "        'confusion': cm.tolist(),\n",
    "        'auc': list(roc_auc.values())\n",
    "    }\n",
    "\n",
    "    if save:\n",
    "        save_evaluation(foldername, name, all_scores, table)\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(classes, y):\n",
    "    class_weight = compute_class_weight(class_weight ='balanced', classes = classes, y = y)\n",
    "    return dict(zip(range(len(classes)),class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    return [\n",
    "        keras.metrics.Accuracy(name=\"Accuracy\"),\n",
    "        keras.metrics.TruePositives(name='True_positives'),\n",
    "        keras.metrics.FalsePositives(name='False_positives'),\n",
    "        keras.metrics.TrueNegatives(name='True_negatives'),\n",
    "        keras.metrics.FalseNegatives(name='False_negatives'), \n",
    "        keras.metrics.BinaryAccuracy(name='Binary_accuracy'),\n",
    "        keras.metrics.Precision(name='Precision'),\n",
    "        keras.metrics.Recall(name='Recall'),\n",
    "        keras.metrics.AUC(name='AUC'),\n",
    "        keras.metrics.AUC(name='PRC', curve='PR'), # precision-recall curve\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, x_train, y_train, x_val, y_val, class_weight, verbose = 1, save = True):\n",
    "    if verbose > 0:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=get_metrics(), weighted_metrics=get_metrics())\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    class_weight = class_weight,\n",
    "                    verbose=verbose)\n",
    "\n",
    "    elapsed_minute = round((time.time() - start_time)/60)\n",
    "    \n",
    "    foldername = None\n",
    "    if save:\n",
    "        foldername = save_model(model, history, elapsed_minute)\n",
    "    plot_execution_history(foldername, history.history, model, verbose, save)\n",
    "      \n",
    "    return model, history, foldername"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foldername = 'classweight'\n",
    "#model, execution = load_model(foldername)\n",
    "#history = execution['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_func, dir_path, dataset_percentage,validation_percentage, verbose, seed = 42):\n",
    "    classes, input_shape, _ = analyse_dataset(dir_path, verbose, seed)\n",
    "    x, y = load_dataset(dir_path, dataset_percentage , verbose)\n",
    "    x_train, x_val, y_train, y_val = split_dataset(x, y, validation_percentage, verbose, seed)\n",
    "    x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "    x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "    model, history, foldername = run_model(model_func(input_shape, classes), x_train_prepared, y_train_prepared, x_val_prepared, y_val_prepared, get_class_weight(classes, y_train), verbose)\n",
    "    history = history.history\n",
    "    train_score = evaluate_model(foldername, 'training',model, x_train_prepared, y_train_prepared, inv_class_map, verbose)\n",
    "    val_score = evaluate_model(foldername, 'validation', model, x_val_prepared, y_val_prepared, inv_class_map, verbose)\n",
    "    return train_score, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reevaluate(results_path, dir_path, dataset_percentage,validation_percentage, verbose, seed = 42):\n",
    "    folders = os.listdir(results_path)\n",
    "    for f in folders:\n",
    "        classes, input_shape, _ = analyse_dataset(dir_path, verbose, seed)\n",
    "        x, y = load_dataset(dir_path, dataset_percentage , verbose)\n",
    "        x_train, x_val, y_train, y_val = split_dataset(x, y, validation_percentage, verbose, seed)\n",
    "        x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "        x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "        model, execution = load_model(f)\n",
    "        _ = execution['history']       \n",
    "        _ = evaluate_model(f, 'training',model, x_train_prepared, y_train_prepared, inv_class_map, verbose)\n",
    "        _ = evaluate_model(f, 'validation', model, x_val_prepared, y_val_prepared, inv_class_map, verbose)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_train_and_evaluate(dir_path, dataset_percentage,validation_percentage, verbose, seed = 42):\n",
    "    stacked_models_0 = [  \n",
    "                        classweight,\n",
    "                        classweight_3_conv2d_layers,\n",
    "                        classweight_4_conv2d_layers,\n",
    "                        classweight_5_conv2d_layers,\n",
    "                        classweight_6_conv2d_layers,\n",
    "                        classweight_7_conv2d_layers,\n",
    "                    ]\n",
    "\n",
    "    stacked_models_1 = [  \n",
    "                        classweight_last_conv2d,\n",
    "                        classweight_last_conv2d_3_conv2d_layers,\n",
    "                        classweight_last_conv2d_4_conv2d_layers,\n",
    "                        classweight_last_conv2d_5_conv2d_layers,\n",
    "                        classweight_last_conv2d_6_conv2d_layers,\n",
    "                        classweight_last_conv2d_7_conv2d_layers,\n",
    "                ]\n",
    "    for m in stacked_models_1:\n",
    "        train_and_evaluate(m, dir_path, dataset_percentage,validation_percentage, verbose, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dir_path = '../Alzheimer_s Dataset/train'\n",
    "model_name = 'testing'\n",
    "results_path = 'results'\n",
    "verbose = 0\n",
    "dataset_percentage = 1\n",
    "validation_percentage = 0.2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading and preparing training dataset\n",
    "#classes, input_shape, class_dist = analyse_dataset(dir_path, verbose, seed)\n",
    "#x, y = load_dataset(dir_path, dataset_percentage , verbose)\n",
    "#x_train, x_val, y_train, y_val = split_dataset(x, y, validation_percentage, verbose, seed)\n",
    "#x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "#x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "## Executing\n",
    "#model, history, foldername = run_model(get_model(input_shape, classes), x_train_prepared, y_train_prepared, x_val_prepared,y_val_prepared, get_class_weight(classes, y_train), verbose)\n",
    "#history = history.history\n",
    "\n",
    "## Loading\n",
    "#foldername = 'classweight [2021-11-18-02-52-58-419281]'\n",
    "#model, execution = load_model(foldername)\n",
    "#history = execution['history']\n",
    "\n",
    "## Evaluating CNN on training dataset\n",
    "#train_score = evaluate_model(foldername, 'training',model, x_train_prepared, y_train_prepared, inv_class_map, verbose)\n",
    "## Evaluating CNN on validation dataset\n",
    "#val_score = evaluate_model(foldername, 'validation', model, x_val_prepared, y_val_prepared, inv_class_map, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked_train_and_evaluate(dir_path, dataset_percentage,validation_percentage, verbose, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reevaluate(results_path, dir_path, dataset_percentage,validation_percentage, verbose, seed)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
