{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "from matplotlib import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from ipywidgets import widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(dir_path):\n",
    "    return os.listdir(dir_path)\n",
    "\n",
    "def data_analysis_histogram(dir_path, classes, verbose = 1):\n",
    "    class_dist = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        class_dist.append(len(os.listdir(class_path)))\n",
    "    \n",
    "    if verbose > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"Class distribution\")\n",
    "        plt.barh(classes, class_dist)\n",
    "        for index, value in enumerate(class_dist):\n",
    "            plt.text(value, index,str(value))\n",
    "        plt.show()\n",
    "\n",
    "def data_analysis_image_size(dir_path, classes, verbose = 1, seed = 42):\n",
    "    random.seed(seed)\n",
    "    random_class_path = os.path.join(dir_path,random.choice(classes))\n",
    "    random_img_name = random.choice(os.listdir(random_class_path))\n",
    "    random_img_path = os.path.join(random_class_path,random_img_name)\n",
    "    img = image.imread(random_img_path)\n",
    "    if verbose > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"%s - Height: %d px x Length: %d px\" % (random_img_path,img.shape[0],img.shape[1]))\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    return (img.shape[0],img.shape[1],1)\n",
    "\n",
    "def analyse_dataset(dir_path, verbose = 1, seed = 42):\n",
    "    classes = get_classes(dir_path)\n",
    "    data_analysis_histogram(dir_path,classes, verbose)\n",
    "    input_shape = data_analysis_image_size(dir_path,classes, verbose, seed)\n",
    "    return classes, input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_path, percentage = 1, verbose = 1):\n",
    "    classes = get_classes(dir_path)\n",
    "    img_array = []\n",
    "    class_array = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        imgs_name = os.listdir(class_path)\n",
    "\n",
    "        if percentage < 1:\n",
    "            imgs_name = random.sample(imgs_name, k = int(len(imgs_name)*percentage))\n",
    "\n",
    "        for i in imgs_name:\n",
    "            img_array.append(image.imread(os.path.join(class_path,i)))\n",
    "            class_array.append(c)\n",
    "    if verbose > 0:\n",
    "        print(\"Loaded %d images\" % len(img_array))\n",
    "    return np.array(img_array), np.array(class_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, val_size = 0.2, verbose = 1, seed = 42):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,  y, test_size=val_size, random_state=seed)\n",
    "    if verbose > 0:\n",
    "        print(\"Train size: %d\\nValidation size: %d\" % (len(x_train), len(x_val)))\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_channel_position(x, input_shape):\n",
    "    img_lin,img_col,n_channels = input_shape\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        x = x.reshape(x.shape[0], n_channels, img_lin, img_col)\n",
    "        input_shape = (n_channels, img_lin, img_col)\n",
    "    else:\n",
    "        x = x.reshape(x.shape[0], img_lin, img_col, n_channels)\n",
    "        input_shape = (img_lin, img_col, n_channels)\n",
    "    return x, input_shape\n",
    "\n",
    "def prepare_dataset_input(x, input_shape):\n",
    "    x_scaled = x.astype('float32') / 255.0\n",
    "    return prepare_dataset_channel_position(x_scaled, input_shape)\n",
    "\n",
    "def prepare_dataset_output(y, classes):\n",
    "    class_map = {x: i for i,x in enumerate(classes)}\n",
    "    y_code = [class_map[word] for word in y]\n",
    "    y_categorical = keras.utils.to_categorical(y_code, len(classes))\n",
    "    inv_class_map = {v: k for k, v in class_map.items()}\n",
    "    return y_categorical, inv_class_map\n",
    "\n",
    "def prepare_dataset(x , y , classes, input_shape):\n",
    "    x_scaled, input_shape = prepare_dataset_input(x, input_shape)\n",
    "    y_categorical, inv_class_map = prepare_dataset_output(y, classes)\n",
    "    return x_scaled , y_categorical, inv_class_map, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-176-3a0981f86cc1>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-176-3a0981f86cc1>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    return scoreTrain, scoreValidation\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_loss():\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_train, x_val, y_train, y_val,  history, verbose = 1):\n",
    "    scoreTrain = model.evaluate(x_train, y_train, verbose = verbose)\n",
    "    scoreValidation = model.evaluate(x_val, y_val, verbose = verbose)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"Training loss: %.4f\" % (scoreTrain))\n",
    "        print(\"Validation loss: %.4f\" % (scoreValidation))\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.plot(history.history['loss'], label=\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    return scoreTrain, scoreValidation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(model, history, dir_path = 'results'):\n",
    "    results_directory = os.path.join(dir_path)\n",
    "\n",
    "    if not os.path.exists(results_directory):\n",
    "        os.makedirs(results_directory)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    now_str = now.strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "\n",
    "    result_directory = os.path.join(results_directory,now_str)\n",
    "\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "    else:\n",
    "        raise ValueError(\"File already exists.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model.save(model_path)\n",
    "\n",
    "    evaluation_path = os.path.join(result_directory,'evaluation')\n",
    "\n",
    "    evaluation = {\n",
    "            'epochs': history.params['epochs'],\n",
    "            'history': history.history\n",
    "    }\n",
    "\n",
    "    with open(evaluation_path, 'wb') as f:\n",
    "        pickle.dump(evaluation, f)\n",
    "    \n",
    "    print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result(foldername, dir_path = 'results'):\n",
    "    result_directory = os.path.join(dir_path,foldername)\n",
    "    if not os.path.exists(result_directory):\n",
    "        raise ValueError(\"Folder not found.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model = keras.models.load_model(model_path)\n",
    "\n",
    "    evaluation_path = os.path.join(result_directory,'evaluation')\n",
    "    evaluation = pickle.load(open(evaluation_path, \"rb\"))\n",
    "\n",
    "    print(\"Loaded!\")\n",
    "    return model, evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_dataset(dir_path, val_size = 0.2, dataset_percentage = 1, verbose = 1, seed = 42):\n",
    "    classes, input_shape = analyse_dataset(dir_path, verbose, seed)\n",
    "    x, y = load_dataset(dir_path, dataset_percentage ,verbose)\n",
    "    x_train, x_val, y_train, y_val = split_dataset(x, y, val_size, verbose, seed)\n",
    "    x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "    x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "    return x_train_prepared, x_val_prepared, y_train_prepared, y_val_prepared, inv_class_map, input_shape, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_A(input_shape, classes, verbose = 1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1),  padding='same', activation='relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(len(classes), activation='softmax'))\n",
    "\n",
    "    if verbose > 0:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "\n",
    "    history = model.fit(x_train_prepared, y_train_prepared,\n",
    "                    batch_size=128,\n",
    "                    epochs=100, verbose=verbose)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "dir_path = '../Alzheimer_s Dataset/train'\n",
    "verbose_loading = 0\n",
    "verbose_training = 1\n",
    "dataset_percentage = 1\n",
    "validation_percentage = 0.2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading and preparing training dataset\n",
    "classes, input_shape = analyse_dataset(dir_path, verbose_loading, seed)\n",
    "x, y = load_dataset(dir_path, dataset_percentage , verbose_loading)\n",
    "x_train, x_val, y_train, y_val = split_dataset(x, y, validation_percentage, verbose_loading, seed)\n",
    "x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing\n",
    "model, history = run_cnn_A(input_shape, classes, verbose_training)\n",
    "save_result(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating CNN\n",
    "evaluate_model(model, x_train_prepared, x_val_prepared, y_train_prepared, y_val_prepared,  history, verbose_training)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
