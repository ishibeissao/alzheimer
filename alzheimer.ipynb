{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "from matplotlib import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from prettytable import PrettyTable \n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.metrics import auc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN MODELS \n",
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITIONS\n",
    "TRAIN_DIR_PATH = '../Alzheimer_s Dataset/train'\n",
    "TEST_DIR_PATH = '../Alzheimer_s Dataset/test'\n",
    "RESULT_PATH = 'results'\n",
    "SEED = 42\n",
    "DATASET_PERCENTAGE = 1\n",
    "VALIDATION_PERCENTAGE = 0\n",
    "N_CLASSES = 4\n",
    "VERBOSE = {\n",
    "    'ANALYSIS': False,\n",
    "    'LOADING': False,\n",
    "    'SPLITTING': False,\n",
    "    'EXECUTION': False,\n",
    "    'EVALUATION': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis_histogram(classes):\n",
    "    class_dist = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(TRAIN_DIR_PATH,c)\n",
    "        class_dist.append(len(os.listdir(class_path)))\n",
    "    \n",
    "    if VERBOSE['ANALYSIS']:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"Class distribution\")\n",
    "        plt.barh(classes, class_dist)\n",
    "        for index, value in enumerate(class_dist):\n",
    "            plt.text(value, index,str(value))\n",
    "        plt.show()\n",
    "    return class_dist\n",
    "\n",
    "def data_analysis_image_size(classes):\n",
    "    random.seed(SEED)\n",
    "    random_class_path = os.path.join(TRAIN_DIR_PATH,random.choice(classes))\n",
    "    random_img_name = random.choice(os.listdir(random_class_path))\n",
    "    random_img_path = os.path.join(random_class_path,random_img_name)\n",
    "    img = image.imread(random_img_path)\n",
    "    if VERBOSE['ANALYSIS']:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.title(\"%s - Height: %d px x Length: %d px\" % (random_img_path,img.shape[0],img.shape[1]))\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    return (img.shape[0],img.shape[1],1)\n",
    "\n",
    "def analyse_dataset():\n",
    "    classes = os.listdir(TRAIN_DIR_PATH)\n",
    "    class_dist = data_analysis_histogram(classes)\n",
    "    input_shape = data_analysis_image_size(classes)\n",
    "    return classes, input_shape, class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_path):\n",
    "    classes = os.listdir(TRAIN_DIR_PATH)\n",
    "    img_array = []\n",
    "    class_array = []\n",
    "    for c in classes:\n",
    "        class_path = os.path.join(dir_path,c)\n",
    "        imgs_name = os.listdir(class_path)\n",
    "\n",
    "        if DATASET_PERCENTAGE < 1:\n",
    "            imgs_name = random.sample(imgs_name, k = int(len(imgs_name)*DATASET_PERCENTAGE))\n",
    "\n",
    "        for i in imgs_name:\n",
    "            img_array.append(image.imread(os.path.join(class_path,i)))\n",
    "            class_array.append(c)\n",
    "    if VERBOSE['LOADING']:\n",
    "        print(\"Loaded %d images\" % len(img_array))\n",
    "        \n",
    "    return np.array(img_array), np.array(class_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y):\n",
    "    if VALIDATION_PERCENTAGE <= 0:\n",
    "        return x, None, y, None\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,  y, test_size=VALIDATION_PERCENTAGE, random_state=SEED)\n",
    "    if VERBOSE['SPLITTING']:\n",
    "        print(\"Train size: %d\\nValidation size: %d\" % (len(x_train), len(x_val)))\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_channel_position(x, input_shape):\n",
    "    img_lin,img_col,n_channels = input_shape\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        x = x.reshape(x.shape[0], n_channels, img_lin, img_col)\n",
    "        input_shape = (n_channels, img_lin, img_col)\n",
    "    else:\n",
    "        x = x.reshape(x.shape[0], img_lin, img_col, n_channels)\n",
    "        input_shape = (img_lin, img_col, n_channels)\n",
    "    return x, input_shape\n",
    "\n",
    "def prepare_dataset_input(x, input_shape):\n",
    "    x_scaled = x.astype('float32') / 255.0\n",
    "    return prepare_dataset_channel_position(x_scaled, input_shape)\n",
    "\n",
    "def prepare_dataset_output(y, classes):\n",
    "    class_map = {x: i for i,x in enumerate(classes)}\n",
    "    y_code = [class_map[word] for word in y]\n",
    "    y_categorical = keras.utils.to_categorical(y_code, len(classes))\n",
    "    inv_class_map = {v: k for k, v in class_map.items()}\n",
    "    return y_categorical, inv_class_map\n",
    "\n",
    "def prepare_dataset(x , y , classes, input_shape):\n",
    "    if x is None:\n",
    "        return None, None, None, None\n",
    "    x_scaled, input_shape = prepare_dataset_input(x, input_shape)\n",
    "    y_categorical, inv_class_map = prepare_dataset_output(y, classes)\n",
    "    return x_scaled , y_categorical, inv_class_map, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, elapsed_minutes):\n",
    "    result_directory = os.path.join(RESULT_PATH)\n",
    "\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "    \n",
    "    name = model.name\n",
    "\n",
    "    result_directory = os.path.join(result_directory,name)\n",
    "\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "    else:\n",
    "        raise ValueError(\"File already exists.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model.save(model_path)\n",
    "\n",
    "    execution_path = os.path.join(result_directory,'execution')\n",
    "\n",
    "    execution = {\n",
    "            'epochs': history.params['epochs'],\n",
    "            'history': history.history,\n",
    "            'elapsed_minutes': elapsed_minutes\n",
    "    }\n",
    "\n",
    "    with open(execution_path, 'wb') as f:\n",
    "        pickle.dump(execution, f)\n",
    "    \n",
    "    pretty_model_path = os.path.join(result_directory, 'model_summary.txt')\n",
    "\n",
    "    with open(pretty_model_path,'w') as f:\n",
    "        f.write('Elapsed time: '+str(elapsed_minutes)+' min\\n')\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    result_directory = os.path.join(RESULT_PATH, model_name)\n",
    "    if not os.path.exists(result_directory):\n",
    "        raise ValueError(\"Folder not found.\")\n",
    "    \n",
    "    model_path = os.path.join(result_directory,'model')\n",
    "    model = keras.models.load_model(model_path)\n",
    "\n",
    "    execution_path = os.path.join(result_directory,'execution')\n",
    "    execution = pickle.load(open(execution_path, \"rb\"))\n",
    "    \n",
    "    return model, execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation(model_name, evalution_name, all_scores, table):\n",
    "    result_directory = os.path.join(RESULT_PATH,model_name)\n",
    "    if not os.path.exists(result_directory):\n",
    "        raise ValueError(\"Folder not found.\")\n",
    "    \n",
    "    score_path = os.path.join(result_directory, evalution_name + '_score.json')\n",
    "    with open(score_path, 'w') as f:\n",
    "        json.dump(all_scores, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    pretty_score_path = os.path.join(result_directory, evalution_name + '_score_summary.txt')\n",
    "\n",
    "    with open(pretty_score_path,'w') as f:\n",
    "        f.write(model_name+'\\n')\n",
    "        f.write(table.get_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_roc(evaluation_name, model, x, y, inv_class_map):\n",
    "    y_pred = model.predict(x)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(inv_class_map)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(inv_class_map)):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            label=\"ROC curve of {0} (AUC = {1:0.2f})\".format(inv_class_map[i], roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    result_directory = os.path.join(RESULT_PATH, model.name)\n",
    "    image_path = os.path.join(result_directory,evaluation_name+'_roc_curve.png')\n",
    "    plt.savefig(image_path)\n",
    "\n",
    "    if VERBOSE['EVALUATION']:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "    return fpr, tpr, roc_auc\n",
    "    \n",
    "def evaluate_model_by_class(model, x, y):\n",
    "    def separate_by_class(x, y):\n",
    "        n_classes = y.shape[1]\n",
    "        x_classified = [[] for _ in range(n_classes)]\n",
    "        y_classified = [[] for _ in range(n_classes)]\n",
    "        \n",
    "        for i,img in enumerate(y):\n",
    "            index = np.where(img==1)[0][0]\n",
    "            x_classified[index].append(x[i])\n",
    "            y_classified[index].append(y[i])\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            x_classified[i] = np.array(x_classified[i])\n",
    "            y_classified[i] = np.array(y_classified[i])\n",
    "            \n",
    "        return np.array(x_classified,dtype=object), np.array(y_classified,dtype=object)\n",
    "\n",
    "    x_by_class, y_by_class = separate_by_class(x,y)\n",
    "    \n",
    "    score_by_class = []\n",
    "    for x,y in zip(x_by_class,y_by_class):\n",
    "        score = model.evaluate(x, y, verbose = 1 if VERBOSE['EVALUATION'] else 0)\n",
    "        score_by_class.append(score)\n",
    "\n",
    "    return score_by_class\n",
    "\n",
    "def evaluate_model_confusion_matrix(evaluation_name, model, x, y, inv_class_map):\n",
    "    def undoOneHotEncoding(y, inv_class_map):\n",
    "        return [inv_class_map[i] for i in np.argmax(y, axis = 1)]\n",
    "    \n",
    "    y_pred = model.predict(x)\n",
    "    y_pred_int = undoOneHotEncoding(y_pred, inv_class_map)\n",
    "    y_int = undoOneHotEncoding(y, inv_class_map)\n",
    "    cm = confusion_matrix(y_int, y_pred_int)\n",
    "\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                        index = [inv_class_map[i] for i in range(len(inv_class_map))], \n",
    "                        columns = [inv_class_map[i] for i in range(len(inv_class_map))])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    \n",
    "    results_directory = os.path.join(RESULT_PATH, model.name)\n",
    "    image_path = os.path.join(results_directory,evaluation_name+'_confusion_matrix.png')\n",
    "    plt.savefig(image_path)\n",
    "    if VERBOSE['EVALUATION']:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    return cm\n",
    "\n",
    "def evaluate_model(evaluation_name, model, x, y, inv_class_map):\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    score = model.evaluate(x, y, verbose = 1 if VERBOSE['EVALUATION'] else 0)\n",
    "    score_by_class = evaluate_model_by_class(model, x, y)\n",
    "    cm = evaluate_model_confusion_matrix(evaluation_name, model,x, y, inv_class_map)\n",
    "    _, _, roc_auc = plot_evaluation_roc(evaluation_name, model, x, y, inv_class_map)\n",
    "    table = PrettyTable()\n",
    "\n",
    "    table.add_column(\"Metrics\", model.metrics_names)\n",
    "    table.add_column(\"Global\", np.round(score,4))\n",
    "\n",
    "    for i, s_class in enumerate(score_by_class):\n",
    "        table.add_column(inv_class_map[i], np.round(s_class,4))\n",
    "\n",
    "    if VERBOSE['EVALUATION']:\n",
    "        print()\n",
    "        print(evaluation_name)\n",
    "        print(table)\n",
    "\n",
    "    all_scores = {\n",
    "        'model_name': model.name,\n",
    "        'name': evaluation_name,\n",
    "        'loss': score,\n",
    "        'loss_by_class': score_by_class,\n",
    "        'confusion': cm.tolist(),\n",
    "        'auc': list(roc_auc.values())\n",
    "    }\n",
    "\n",
    "    save_evaluation(model.name, evaluation_name, all_scores, table)\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(classes, y):\n",
    "    class_weight = compute_class_weight(class_weight ='balanced', classes = classes, y = y)\n",
    "    return dict(zip(range(len(classes)),class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    return [\n",
    "        keras.metrics.Accuracy(name=\"Accuracy\"),\n",
    "        keras.metrics.Precision(name='Precision'),\n",
    "        keras.metrics.Recall(name='Recall'),\n",
    "        keras.metrics.AUC(name='AUC'),\n",
    "        keras.metrics.AUC(name='PRC', curve='PR'), # precision-recall curve\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_execution_history(history, model):\n",
    "    result_directory = os.path.join(RESULT_PATH, model.name)\n",
    "    image_path = os.path.join(result_directory,'execution.png')\n",
    "\n",
    "    if VALIDATION_PERCENTAGE > 0:\n",
    "        nplots = len(history.values())/2\n",
    "    else:\n",
    "        nplots = len(history.values())\n",
    "    nrows = int(nplots/3)\n",
    "    ncols = 3\n",
    "    fig = plt.figure(figsize=(ncols*8, nrows*5))\n",
    "    gs = fig.add_gridspec(nrows, ncols)\n",
    "    axs = gs.subplots(sharex=False, sharey=False)\n",
    "    for i,h in enumerate(history.values()):\n",
    "        i_hat = int(i%nplots)\n",
    "        r = int(i_hat/ncols)\n",
    "        c = i_hat%ncols\n",
    "        if VALIDATION_PERCENTAGE > 0:\n",
    "            axs[r,c].plot(h, label = 'Training' if int(i/nplots)==0 else 'Validation')\n",
    "        else:\n",
    "            axs[r,c].plot(h, label = 'Training')\n",
    "        \n",
    "    for i,n in enumerate(model.metrics_names):\n",
    "        r = int(i/ncols)\n",
    "        c = i%ncols\n",
    "        axs[r,c].set_xlabel('Epoch')\n",
    "        axs[r,c].set_ylabel(n)\n",
    "        axs[r,c].legend()\n",
    "      \n",
    "    plt.savefig(image_path)\n",
    "    if VERBOSE['EXECUTION']:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "def run_model(model, x_train, y_train, x_val, y_val, class_weight):\n",
    "    if VERBOSE['EXECUTION']:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=get_metrics(), weighted_metrics=get_metrics())\n",
    "\n",
    "    if x_val is None:\n",
    "        xy_val = None\n",
    "    else:\n",
    "        xy_val = (x_val, y_val)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    validation_data= xy_val,\n",
    "                    class_weight = class_weight,\n",
    "                    verbose=1 if VERBOSE['EXECUTION'] else 0)\n",
    "\n",
    "    elapsed_minute = round((time.time() - start_time)/60)\n",
    "\n",
    "    save_model(model, history, elapsed_minute)\n",
    "    plot_execution_history(history.history, model)\n",
    "      \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(models_func):\n",
    "    classes, input_shape, _ = analyse_dataset()\n",
    "    x_trainval, y_trainval = load_dataset(TRAIN_DIR_PATH)\n",
    "    x_test, y_test = load_dataset(TEST_DIR_PATH)\n",
    "    x_train, x_val, y_train, y_val = split_dataset(x_trainval, y_trainval)\n",
    "    x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "    x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "    x_test_prepared , y_test_prepared, _, _ = prepare_dataset(x_test , y_test , classes, input_shape)\n",
    "    for model_func in models_func:\n",
    "        model, history = run_model(model_func(input_shape, classes), x_train_prepared, y_train_prepared, x_val_prepared, y_val_prepared, get_class_weight(classes, y_train))\n",
    "        history = history.history\n",
    "        evaluate_model('training',model, x_train_prepared, y_train_prepared, inv_class_map)\n",
    "        evaluate_model('validation', model, x_val_prepared, y_val_prepared, inv_class_map)\n",
    "        evaluate_model('test', model, x_test_prepared, y_test_prepared, inv_class_map)\n",
    "\n",
    "\n",
    "def reevaluate():\n",
    "    classes, input_shape, _ = analyse_dataset()\n",
    "    x_trainval, y_trainval = load_dataset(TRAIN_DIR_PATH)\n",
    "    x_test, y_test = load_dataset(TEST_DIR_PATH)\n",
    "    x_train, x_val, y_train, y_val = split_dataset(x_trainval, y_trainval)\n",
    "    x_train_prepared , y_train_prepared, inv_class_map, input_shape = prepare_dataset(x_train , y_train , classes, input_shape)\n",
    "    x_val_prepared , y_val_prepared, _, _ = prepare_dataset(x_val , y_val , classes, input_shape)\n",
    "    x_test_prepared , y_test_prepared, _, _ = prepare_dataset(x_test , y_test , classes, input_shape)\n",
    "    for f in os.listdir(RESULT_PATH):   \n",
    "        model, _ = load_model(f)\n",
    "        evaluate_model('training',model, x_train_prepared, y_train_prepared, inv_class_map)\n",
    "        evaluate_model('validation', model, x_val_prepared, y_val_prepared, inv_class_map)\n",
    "        evaluate_model('test', model, x_test_prepared, y_test_prepared, inv_class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_models_0 = [  \n",
    "                    classweight,\n",
    "                    classweight_3_conv2d_layers,\n",
    "                    classweight_4_conv2d_layers,\n",
    "                    classweight_5_conv2d_layers,\n",
    "                    classweight_6_conv2d_layers,\n",
    "                    classweight_7_conv2d_layers,\n",
    "                ]\n",
    "stacked_models_1 = [  \n",
    "                    classweight_last_conv2d,\n",
    "                    classweight_last_conv2d_3_conv2d_layers,\n",
    "                    classweight_last_conv2d_4_conv2d_layers,\n",
    "                    classweight_last_conv2d_5_conv2d_layers,\n",
    "                    classweight_last_conv2d_6_conv2d_layers,\n",
    "                    classweight_last_conv2d_7_conv2d_layers,\n",
    "                ]\n",
    "\n",
    "stacked_models_2 = [\n",
    "                    classweight_1_conv2d_layers,\n",
    "                    classweight_last_conv2d_1_conv2d_layer,\n",
    "                    classweight_no_pooling_1_conv2d_layer,\n",
    "                    classweight_no_pooling_2_conv2d_layers,\n",
    "                    classweight_no_pooling_3_conv2d_layers\n",
    "                ]\n",
    "#stacked_train_and_evaluate(stacked_models_2)\n",
    "reevaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
